"""
è½¬å½•ç»„æ™ºèƒ½ä½“ï¼ˆRNA Agentï¼‰
å¤„ç†å•ç»†èƒè½¬å½•ç»„ï¼ˆscRNA-seqï¼‰å’Œ Bulk RNA-seq åˆ†æ
é‡æ„è‡ªç°æœ‰çš„ BioBlendAgent
"""
import json
import os
from typing import Dict, Any, List, Optional, AsyncIterator
from ..base_agent import BaseAgent
from ...core.llm_client import LLMClient
from ...core.prompt_manager import PromptManager
from ...core.dispatcher import TaskDispatcher
from ...tools.cellranger_tool import CellRangerTool
from ...tools.scanpy_tool import ScanpyTool


class RNAAgent(BaseAgent):
    """
    è½¬å½•ç»„æ™ºèƒ½ä½“
    
    èŒè´£ï¼š
    1. å¤„ç†å•ç»†èƒè½¬å½•ç»„åˆ†æï¼ˆscRNA-seqï¼‰
    2. å¤„ç† Bulk RNA-seq åˆ†æ
    3. ç”Ÿæˆå·¥ä½œæµè„šæœ¬
    4. é€šè¿‡ TaskDispatcher æäº¤ä»»åŠ¡
    """
    
    def __init__(
        self,
        llm_client: LLMClient,
        prompt_manager: PromptManager,
        dispatcher: Optional[TaskDispatcher] = None,
        cellranger_config: Optional[Dict[str, Any]] = None,
        scanpy_config: Optional[Dict[str, Any]] = None
    ):
        """åˆå§‹åŒ–è½¬å½•ç»„æ™ºèƒ½ä½“"""
        super().__init__(llm_client, prompt_manager, "rna_expert")
        
        self.dispatcher = dispatcher
        self.cellranger_config = cellranger_config or {}
        self.scanpy_config = scanpy_config or {}
        self.cellranger_tool = CellRangerTool(self.cellranger_config)
        self.scanpy_tool = ScanpyTool(self.scanpy_config)
        
        # æ ‡å‡†å·¥ä½œæµæ­¥éª¤ï¼ˆåæ­¥æµç¨‹ï¼‰
        self.workflow_steps = [
            {"name": "1. Quality Control", "tool_id": "local_qc", "desc": "è¿‡æ»¤ä½è´¨é‡ç»†èƒå’ŒåŸºå› "},
            {"name": "2. Normalization", "tool_id": "local_normalize", "desc": "æ•°æ®æ ‡å‡†åŒ–"},
            {"name": "3. Find Variable Genes", "tool_id": "local_hvg", "desc": "ç­›é€‰é«˜å˜åŸºå› "},
            {"name": "4. Scale Data", "tool_id": "local_scale", "desc": "æ•°æ®ç¼©æ”¾"},
            {"name": "5. PCA", "tool_id": "local_pca", "desc": "ä¸»æˆåˆ†åˆ†æ"},
            {"name": "6. Compute Neighbors", "tool_id": "local_neighbors", "desc": "æ„å»ºé‚»æ¥å›¾"},
            {"name": "7. Clustering", "tool_id": "local_cluster", "desc": "Leiden èšç±»"},
            {"name": "8. UMAP Visualization", "tool_id": "local_umap", "desc": "UMAP å¯è§†åŒ–"},
            {"name": "9. t-SNE Visualization", "tool_id": "local_tsne", "desc": "t-SNE å¯è§†åŒ–"},
            {"name": "10. Find Markers", "tool_id": "local_markers", "desc": "å¯»æ‰¾ Marker åŸºå› "},
        ]
    
    async def process_query(
        self,
        query: str,
        history: List[Dict[str, str]] = None,
        uploaded_files: List[Dict[str, str]] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼Œå¯èƒ½åŒ…å«ï¼š
            - workflow_config: å·¥ä½œæµé…ç½®ï¼ˆJSONï¼‰
            - chat_response: èŠå¤©å“åº”ï¼ˆæµå¼ï¼‰
            - task_submitted: ä»»åŠ¡æäº¤ä¿¡æ¯
        """
        query_lower = query.lower().strip()
        file_paths = self.get_file_paths(uploaded_files or [])
        
        # æ„å›¾è¯†åˆ«
        is_workflow_request = self._is_workflow_request(query_lower, file_paths)
        
        if is_workflow_request:
            return await self._generate_workflow_config(query, file_paths)
        else:
            # æ™®é€šèŠå¤©
            return {
                "type": "chat",
                "response": self._stream_chat_response(query, file_paths)
            }
    
    def _is_workflow_request(self, query: str, file_paths: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å·¥ä½œæµè¯·æ±‚"""
        workflow_keywords = [
            "è§„åˆ’", "æµç¨‹", "workflow", "pipeline", "åˆ†æ", "run",
            "æ‰§è¡Œ", "plan", "åšä¸€ä¸‹", "è·‘ä¸€ä¸‹", "åˆ†æä¸€ä¸‹"
        ]
        
        bio_keywords = [
            "pca", "umap", "tsne", "qc", "è´¨æ§", "èšç±»", "cluster"
        ]
        
        if any(kw in query for kw in workflow_keywords):
            return True
        
        if file_paths and any(kw in query for kw in bio_keywords):
            return True
        
        if file_paths and (not query or len(query) < 5):
            return True
        
        return False
    
    async def _generate_workflow_config(
        self,
        query: str,
        file_paths: List[str]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå·¥ä½œæµé…ç½®
        
        å¼ºåˆ¶æµç¨‹ï¼š
        1. å…ˆæ£€æŸ¥æ–‡ä»¶ï¼ˆinspect_fileï¼‰
        2. åŸºäºæ£€æŸ¥ç»“æœæå–å‚æ•°
        3. ç”Ÿæˆå·¥ä½œæµé…ç½®
        """
        # å¼ºåˆ¶æ£€æŸ¥ï¼šå¦‚æœæœ‰æ–‡ä»¶ï¼Œå…ˆæ£€æŸ¥
        inspection_result = None
        if file_paths:
            input_path = file_paths[0]
            try:
                inspection_result = self.scanpy_tool.inspect_file(input_path)
                if "error" in inspection_result:
                    # æ£€æŸ¥å¤±è´¥ï¼Œä½†ä»ç„¶ç»§ç»­ï¼ˆå¯èƒ½æ˜¯æ–‡ä»¶è·¯å¾„é—®é¢˜ï¼‰
                    import logging
                    logger = logging.getLogger(__name__)
                    logger.warning(f"File inspection failed: {inspection_result.get('error')}")
            except Exception as e:
                import logging
                logger = logging.getLogger(__name__)
                logger.error(f"Error inspecting file: {e}", exc_info=True)
        
        # ä½¿ç”¨ LLM æå–å‚æ•°ï¼ˆä¼ å…¥æ£€æŸ¥ç»“æœï¼‰
        extracted_params = await self._extract_workflow_params(query, file_paths, inspection_result)
        
        # æ„å»ºå·¥ä½œæµé…ç½®
        workflow_config = {
            "workflow_name": "Standard Single-Cell Pipeline",
            "steps": []
        }
        
        for step_template in self.workflow_steps:
            step = step_template.copy()
            
            # æ³¨å…¥å‚æ•°
            tool_id = step["tool_id"]
            if tool_id == "local_qc":
                step["params"] = {
                    "min_genes": extracted_params.get("min_genes", "200"),
                    "max_mt": extracted_params.get("max_mt", "20")
                }
            elif tool_id == "local_hvg":
                step["params"] = {
                    "n_top_genes": extracted_params.get("n_top_genes", "2000")
                }
            elif tool_id == "local_cluster":
                step["params"] = {
                    "resolution": extracted_params.get("resolution", "0.5")
                }
            else:
                step["params"] = {}
            
            workflow_config["steps"].append(step)
        
        return {
            "type": "workflow_config",
            "workflow_data": workflow_config,
            "file_paths": file_paths
        }
    
    async def _extract_workflow_params(
        self,
        query: str,
        file_paths: List[str],
        inspection_result: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM æå–å·¥ä½œæµå‚æ•°
        
        åŸºäºæ£€æŸ¥ç»“æœæ™ºèƒ½æ¨èå‚æ•°
        """
        # æ„å»ºåŒ…å«æ£€æŸ¥ç»“æœçš„æç¤º
        inspection_info = ""
        if inspection_result and "error" not in inspection_result:
            inspection_info = f"""
ã€Data Inspection Resultsã€‘
- Number of cells (n_obs): {inspection_result.get('n_obs', 'N/A')}
- Number of genes (n_vars): {inspection_result.get('n_vars', 'N/A')}
- Max value: {inspection_result.get('max_value', 'N/A')}
- Is normalized: {inspection_result.get('is_normalized', False)}
- Has QC metrics: {inspection_result.get('has_qc_metrics', False)}
- Has clusters: {inspection_result.get('has_clusters', False)}
- Has UMAP: {inspection_result.get('has_umap', False)}

ã€Recommendations Based on Inspectionã€‘
"""
            n_obs = inspection_result.get('n_obs', 0)
            is_normalized = inspection_result.get('is_normalized', False)
            has_qc = inspection_result.get('has_qc_metrics', False)
            
            if n_obs > 10000:
                inspection_info += "- Large dataset (>10k cells): Recommend min_genes=500, max_mt=5%\n"
            elif n_obs > 5000:
                inspection_info += "- Medium dataset (5k-10k cells): Recommend min_genes=300, max_mt=5%\n"
            else:
                inspection_info += "- Small dataset (<5k cells): Recommend min_genes=200, max_mt=10%\n"
            
            if is_normalized:
                inspection_info += "- Data appears normalized: Skip normalization step\n"
            else:
                inspection_info += "- Data appears to be raw counts: Need normalization\n"
            
            if has_qc:
                inspection_info += "- QC metrics already calculated: May skip QC calculation\n"
        
        prompt = f"""Extract workflow parameters from user query and inspection results:

Query: {query}
Files: {', '.join(file_paths) if file_paths else 'None'}
{inspection_info}

Extract these parameters (if mentioned in query, otherwise use recommendations):
- min_genes (default: 200, adjust based on dataset size)
- max_mt (default: 20, adjust based on dataset size)
- resolution (default: 0.5, for clustering)
- n_top_genes (default: 2000, for HVG selection)

Return JSON only:
{{"resolution": "0.8", "min_genes": "500", "max_mt": "5"}}
"""
        
        messages = [
            {"role": "system", "content": "You are a parameter extraction assistant. Return JSON only."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            completion = await self.llm_client.achat(messages, temperature=0.1, max_tokens=256)
            # æå– think è¿‡ç¨‹å’Œå®é™…å†…å®¹
            think_content, response = self.llm_client.extract_think_and_content(completion)
            # å¦‚æœæœ‰ think å†…å®¹ï¼Œè®°å½•æ—¥å¿—ï¼ˆå¯é€‰ï¼‰
            if think_content:
                import logging
                logger = logging.getLogger(__name__)
                logger.debug(f"RNA Agent think process: {think_content[:200]}...")
            
            # è§£æ JSON
            json_str = response.strip()
            if "```json" in json_str:
                json_str = json_str.split("```json")[1].split("```")[0].strip()
            
            return json.loads(json_str)
        except:
            return {}
    
    async def _stream_chat_response(
        self,
        query: str,
        file_paths: List[str]
    ) -> AsyncIterator[str]:
        """
        æµå¼èŠå¤©å“åº”ï¼ˆæ”¯æŒ ReAct å¾ªç¯å’Œå·¥å…·è°ƒç”¨ï¼‰
        
        å®ç° ReAct å¾ªç¯ï¼š
        1. Thought: LLM æ€è€ƒ
        2. Action: è°ƒç”¨å·¥å…·ï¼ˆå¦‚ inspect_fileï¼‰
        3. Observation: å·¥å…·è¿”å›ç»“æœ
        4. Final Answer: æœ€ç»ˆå›ç­”
        """
        context = {
            "context": f"Uploaded files: {', '.join(file_paths) if file_paths else 'None'}",
            "available_tools": ["inspect_file"],
            "tool_descriptions": {
                "inspect_file": "æ£€æŸ¥æ•°æ®æ–‡ä»¶ï¼Œè¿”å›æ•°æ®æ‘˜è¦ï¼ˆn_obs, n_vars, obs_keys, var_keys, is_normalized, etc.ï¼‰"
            }
        }
        
        # å¦‚æœæœ‰æ–‡ä»¶ï¼Œå¼ºåˆ¶å…ˆæ£€æŸ¥ï¼ˆç¬¦åˆ SOPï¼‰
        inspection_result = None
        if file_paths:
            input_path = file_paths[0]
            try:
                inspection_result = self.scanpy_tool.inspect_file(input_path)
                if "error" not in inspection_result:
                    # å°†æ£€æŸ¥ç»“æœæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
                    inspection_summary = f"""
ã€Data Inspection Completedã€‘
- Cells: {inspection_result.get('n_obs', 'N/A')}
- Genes: {inspection_result.get('n_vars', 'N/A')}
- Max value: {inspection_result.get('max_value', 'N/A')}
- Normalized: {inspection_result.get('is_normalized', False)}
- Has QC metrics: {inspection_result.get('has_qc_metrics', False)}
- Has clusters: {inspection_result.get('has_clusters', False)}
"""
                    # å…ˆè¾“å‡ºæ£€æŸ¥ç»“æœ
                    yield f"ğŸ” **Data Inspection Results:**\n{inspection_summary}\n\n"
                    # å°†æ£€æŸ¥ç»“æœæ·»åŠ åˆ°æŸ¥è¯¢ä¸­ï¼Œè®© LLM åŸºäºæ­¤åˆ†æ
                    query = f"""{query}

{inspection_summary}

Based on the inspection results above, please:
1. Analyze the data characteristics
2. Propose appropriate analysis parameters
3. Ask for confirmation before proceeding with analysis
"""
            except Exception as e:
                import logging
                logger = logging.getLogger(__name__)
                logger.error(f"Error inspecting file: {e}", exc_info=True)
                yield f"âš ï¸ Warning: Could not inspect file: {str(e)}\n\n"
        
        # æ„å»ºå¢å¼ºçš„ç”¨æˆ·æŸ¥è¯¢ï¼ŒåŒ…å«å·¥å…·è¯´æ˜
        enhanced_query = f"""{query}

ã€Available Toolsã€‘
You have access to: inspect_file(file_path) - already executed above if files were provided.

ã€Workflow Ruleã€‘
Before running any analysis, you MUST have inspected the data first (already done above).
Now analyze the inspection results and propose parameters.
"""
        
        # æµå¼è¾“å‡º LLM å“åº”
        async for chunk in self.chat(enhanced_query, context, stream=True):
            yield chunk
    
    async def execute_workflow(
        self,
        workflow_config: Dict[str, Any],
        file_paths: List[str],
        output_dir: str
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå·¥ä½œæµ
        
        æ ¸å¿ƒï¼šç›´æ¥æ‰§è¡Œ scanpy åˆ†ææµç¨‹ï¼ˆå‚è€ƒæ—§ç‰ˆæœ¬å®ç°ï¼‰
        
        Args:
            workflow_config: å·¥ä½œæµé…ç½®
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        
        Returns:
            åˆ†ææŠ¥å‘Š
        """
        # æ£€æµ‹è¾“å…¥æ–‡ä»¶ç±»å‹
        input_path = file_paths[0] if file_paths else None
        if not input_path:
            raise ValueError("No input files provided")
        
        file_type = self.detect_file_type(input_path)
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        # æ›´æ–° scanpy å·¥å…·çš„è¾“å‡ºç›®å½•
        self.scanpy_config["output_dir"] = output_dir
        # é‡æ–°åˆå§‹åŒ– scanpy å·¥å…·ä»¥ä½¿ç”¨æ–°çš„è¾“å‡ºç›®å½•
        self.scanpy_tool = ScanpyTool(self.scanpy_config)
        
        # ç›´æ¥æ‰§è¡Œ Scanpy æµç¨‹
        if file_type == "fastq":
            # éœ€è¦å…ˆè¿è¡Œ Cell Rangerï¼ˆæš‚ä¸æ”¯æŒï¼Œå…ˆè·³è¿‡ï¼‰
            raise NotImplementedError("Cell Ranger preprocessing is not yet implemented")
        else:
            # ç›´æ¥è¿è¡Œ Scanpy åˆ†æ
            steps = workflow_config.get("steps", [])
            
            # æ‰§è¡Œåˆ†ææµç¨‹
            report = self.scanpy_tool.run_pipeline(
                data_input=input_path,
                steps_config=steps
            )
            
            return report

