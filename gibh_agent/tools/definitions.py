"""
å·¥å…·å®šä¹‰ç¤ºä¾‹ - ä½¿ç”¨ ToolRegistry æ ‡å‡†åŒ–å·¥å…·

å±•ç¤ºå¦‚ä½•ä½¿ç”¨è£…é¥°å™¨ç³»ç»Ÿå®šä¹‰å’Œæ³¨å†Œå·¥å…·ã€‚
è¿™äº›å·¥å…·å°†è¢«ç”¨äºåŠ¨æ€ Tool-RAG æ¶æ„ã€‚
"""
import os
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional
from pathlib import Path
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import logging

from ..core.tool_registry import registry

logger = logging.getLogger(__name__)


# ============================================================================
# ä»£è°¢ç»„å­¦å·¥å…·å®šä¹‰ç¤ºä¾‹
# ============================================================================

@registry.register(
    name="metabolomics_pca",
    description="Performs Principal Component Analysis (PCA) on metabolite abundance data. Returns PCA coordinates, explained variance, and optionally a PCA plot.",
    category="Metabolomics",
    output_type="mixed"  # è¿”å› JSON + å›¾ç‰‡è·¯å¾„
)
def run_pca(
    file_path: str,
    n_components: int = 2,
    scale: bool = True,
    output_dir: Optional[str] = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œ PCA åˆ†æ
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼‰
        n_components: ä¸»æˆåˆ†æ•°é‡ï¼ˆé»˜è®¤ 2ï¼‰
        scale: æ˜¯å¦æ ‡å‡†åŒ–æ•°æ®ï¼ˆé»˜è®¤ Trueï¼‰
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - pca_coordinates: PCA åæ ‡ (DataFrame çš„ JSON è¡¨ç¤º)
        - explained_variance: è§£é‡Šæ–¹å·®æ¯”ä¾‹
        - plot_path: PCA å›¾è·¯å¾„ï¼ˆå¦‚æœç”Ÿæˆï¼‰
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # æå–æ•°å€¼åˆ—ï¼ˆæ’é™¤éæ•°å€¼åˆ—ï¼‰
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        data = df[numeric_cols]
        
        # ğŸ”¥ æ£€æŸ¥æ•°æ®ç»´åº¦
        n_samples, n_features = data.shape
        if n_features < 2:
            return {
                "status": "error",
                "error": f"PCA éœ€è¦è‡³å°‘ 2 ä¸ªç‰¹å¾ï¼Œä½†æ•°æ®åªæœ‰ {n_features} ä¸ªç‰¹å¾ã€‚è¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†æ­¥éª¤æ˜¯å¦æ­£ç¡®ä¿ç•™äº†ä»£è°¢ç‰©åˆ—ã€‚",
                "data_shape": {"rows": n_samples, "columns": n_features}
            }
        
        # ğŸ”¥ è‡ªåŠ¨è°ƒæ•´ n_componentsï¼ˆä¸èƒ½è¶…è¿‡ min(n_samples, n_features)ï¼‰
        max_components = min(n_samples, n_features)
        actual_n_components = min(n_components, max_components)
        
        if actual_n_components < n_components:
            logger.warning(f"âš ï¸ è¯·æ±‚çš„ n_components={n_components} è¶…è¿‡æ•°æ®ç»´åº¦é™åˆ¶ (min({n_samples}, {n_features})={max_components})ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º {actual_n_components}")
        
        # æ•°æ®é¢„å¤„ç†
        if scale:
            scaler = StandardScaler()
            data_scaled = scaler.fit_transform(data)
        else:
            data_scaled = data.values
        
        # æ‰§è¡Œ PCA
        pca = PCA(n_components=actual_n_components)
        pca_coords = pca.fit_transform(data_scaled)
        
        # åˆ›å»ºç»“æœ DataFrame
        coords_df = pd.DataFrame(
            pca_coords,
            index=data.index,
            columns=[f"PC{i+1}" for i in range(actual_n_components)]
        )
        
        # ç”Ÿæˆå›¾ç‰‡ï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼‰
        plot_path = None
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            plot_path = str(output_path / "pca_plot.png")
            
            plt.figure(figsize=(10, 8))
            plt.scatter(coords_df.iloc[:, 0], coords_df.iloc[:, 1], alpha=0.6)
            plt.xlabel(f"PC1 ({pca.explained_variance_ratio_[0]:.2%})")
            plt.ylabel(f"PC2 ({pca.explained_variance_ratio_[1]:.2%})")
            plt.title("PCA Plot")
            plt.grid(True, alpha=0.3)
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
        
        return {
            "status": "success",
            "pca_coordinates": coords_df.to_dict(orient='index'),
            "explained_variance": {
                f"PC{i+1}": float(ratio) 
                for i, ratio in enumerate(pca.explained_variance_ratio_)
            },
            "plot_path": plot_path,
            "n_components": actual_n_components,
            "requested_n_components": n_components,
            "data_shape": {"rows": n_samples, "columns": n_features}
        }
    
    except Exception as e:
        logger.error(f"âŒ PCA åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }


@registry.register(
    name="metabolomics_differential_analysis",
    description="Performs differential analysis between two groups in metabolite data. Uses t-test to identify significantly different metabolites. Returns p-values, FDR-corrected p-values, and log2 fold changes.",
    category="Metabolomics",
    output_type="json"
)
def run_differential_analysis(
    file_path: str,
    group_column: str,
    case_group: str,
    control_group: str,
    fdr_method: str = "fdr_bh"
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå·®å¼‚ä»£è°¢ç‰©åˆ†æ
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼ŒåŒ…å«åˆ†ç»„ä¿¡æ¯ï¼‰
        group_column: åˆ†ç»„åˆ—å
        case_group: å®éªŒç»„åç§°
        control_group: å¯¹ç…§ç»„åç§°
        fdr_method: FDR æ ¡æ­£æ–¹æ³•ï¼ˆé»˜è®¤ "fdr_bh"ï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - results: å·®å¼‚åˆ†æç»“æœåˆ—è¡¨ï¼ˆæ¯ä¸ªä»£è°¢ç‰©ä¸€è¡Œï¼‰
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        from scipy import stats
        from statsmodels.stats.multitest import multipletests
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # æ£€æŸ¥åˆ†ç»„åˆ—æ˜¯å¦å­˜åœ¨
        if group_column not in df.columns:
            return {
                "status": "error",
                "error": f"åˆ†ç»„åˆ— '{group_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­"
            }
        
        # åˆ†ç¦»åˆ†ç»„
        groups = df[group_column]
        case_mask = groups == case_group
        control_mask = groups == control_group
        
        if not case_mask.any():
            return {
                "status": "error",
                "error": f"å®éªŒç»„ '{case_group}' ä¸å­˜åœ¨"
            }
        if not control_mask.any():
            return {
                "status": "error",
                "error": f"å¯¹ç…§ç»„ '{control_group}' ä¸å­˜åœ¨"
            }
        
        # æå–ä»£è°¢ç‰©åˆ—ï¼ˆæ•°å€¼åˆ—ï¼Œæ’é™¤åˆ†ç»„åˆ—ï¼‰
        metabolite_cols = [
            col for col in df.columns 
            if col != group_column and pd.api.types.is_numeric_dtype(df[col])
        ]
        
        results = []
        p_values = []
        
        for metabolite in metabolite_cols:
            case_values = df.loc[case_mask, metabolite].dropna()
            control_values = df.loc[control_mask, metabolite].dropna()
            
            if len(case_values) < 2 or len(control_values) < 2:
                continue
            
            # T-test
            t_stat, p_val = stats.ttest_ind(case_values, control_values)
            
            # è®¡ç®— log2 fold change
            case_mean = case_values.mean()
            control_mean = control_values.mean()
            
            if control_mean > 0:
                log2fc = np.log2(case_mean / control_mean)
            else:
                log2fc = 0.0
            
            results.append({
                "metabolite": metabolite,
                "p_value": float(p_val),
                "log2fc": float(log2fc),
                "case_mean": float(case_mean),
                "control_mean": float(control_mean)
            })
            p_values.append(p_val)
        
        # FDR æ ¡æ­£
        if p_values:
            _, p_adjusted, _, _ = multipletests(p_values, method=fdr_method)
            
            # æ·»åŠ  FDR æ ¡æ­£åçš„ p å€¼
            for i, result in enumerate(results):
                result["fdr"] = float(p_adjusted[i])
                result["significant"] = p_adjusted[i] < 0.05
        
        return {
            "status": "success",
            "results": results,
            "summary": {
                "total_metabolites": len(results),
                "significant_count": sum(1 for r in results if r.get("significant", False))
            }
        }
    
    except Exception as e:
        logger.error(f"âŒ å·®å¼‚åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }


@registry.register(
    name="metabolomics_preprocess",
    description="Preprocesses metabolite data: handles missing values, applies log2 transformation, and standardizes the data. Returns preprocessed DataFrame.",
    category="Metabolomics",
    output_type="json"
)
def preprocess_metabolite_data(
    file_path: str,
    missing_imputation: str = "min",
    log_transform: bool = True,
    standardize: bool = True
) -> Dict[str, Any]:
    """
    é¢„å¤„ç†ä»£è°¢ç‰©æ•°æ®
    
    Args:
        file_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆCSVï¼‰
        missing_imputation: ç¼ºå¤±å€¼å¡«å……æ–¹æ³•ï¼ˆ"min", "median", "mean", "zero"ï¼‰
        log_transform: æ˜¯å¦è¿›è¡Œ log2 è½¬æ¢ï¼ˆé»˜è®¤ Trueï¼‰
        standardize: æ˜¯å¦æ ‡å‡†åŒ–ï¼ˆé»˜è®¤ Trueï¼‰
    
    Returns:
        åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
        - status: "success" æˆ– "error"
        - preprocessed_data: é¢„å¤„ç†åçš„æ•°æ®ï¼ˆJSON æ ¼å¼ï¼‰
        - output_path: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¿å­˜ï¼‰
        - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        # è¯»å–æ•°æ®
        df = pd.read_csv(file_path, index_col=0)
        
        # æå–æ•°å€¼åˆ—
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        data = df[numeric_cols].copy()
        
        # 1. ç¼ºå¤±å€¼å¡«å……
        if missing_imputation == "min":
            data = data.fillna(data.min())
        elif missing_imputation == "median":
            data = data.fillna(data.median())
        elif missing_imputation == "mean":
            data = data.fillna(data.mean())
        else:  # "zero"
            data = data.fillna(0)
        
        # 2. Log2 è½¬æ¢
        if log_transform:
            data = data.apply(lambda x: np.log2(x + 1))
        
        # 3. æ ‡å‡†åŒ–
        if standardize:
            scaler = StandardScaler()
            data_scaled = scaler.fit_transform(data)
            data = pd.DataFrame(
                data_scaled,
                index=data.index,
                columns=data.columns
            )
        
        return {
            "status": "success",
            "preprocessed_data": data.to_dict(orient='index'),
            "shape": {
                "rows": len(data),
                "columns": len(data.columns)
            }
        }
    
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }


# ============================================================================
# é€šç”¨å·¥å…·å®šä¹‰ç¤ºä¾‹
# ============================================================================

@registry.register(
    name="file_inspect",
    description="Inspects a file and returns basic metadata: file size, type, number of rows/columns (for tabular files), and a preview of the first few rows.",
    category="General",
    output_type="json"
)
def inspect_file(
    file_path: str,
    preview_rows: int = 5
) -> Dict[str, Any]:
    """
    æ£€æŸ¥æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        preview_rows: é¢„è§ˆè¡Œæ•°ï¼ˆé»˜è®¤ 5ï¼‰
    
    Returns:
        åŒ…å«æ–‡ä»¶å…ƒæ•°æ®çš„å­—å…¸
    """
    try:
        path = Path(file_path)
        
        if not path.exists():
            return {
                "status": "error",
                "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            }
        
        metadata = {
            "status": "success",
            "file_path": str(path),
            "file_size": path.stat().st_size,
            "file_type": path.suffix,
            "exists": True
        }
        
        # å¦‚æœæ˜¯ CSV æ–‡ä»¶ï¼Œè¯»å–é¢„è§ˆ
        if path.suffix.lower() == '.csv':
            df = pd.read_csv(file_path, nrows=preview_rows)
            metadata.update({
                "rows": len(df),
                "columns": list(df.columns),
                "preview": df.head(preview_rows).to_dict(orient='records')
            })
        
        return metadata
    
    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e)
        }

